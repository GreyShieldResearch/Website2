file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"  # Update with your exact file path
pdf_text <- pdf_text(file_path)  # Extract text from the PDF
# Step 2: Parse text into lines
lines <- unlist(str_split(pdf_text, "\\n"))  # Split text by lines
# Step 3: Detect header rows and align data
# Initialize variables
data <- tibble()
institution <- NULL
headers <- NULL
rows <- list()
for (line in lines) {
# Detect institution and reset headers
if (str_detect(line, "COMPSTAT Incident Report - Institution:")) {
institution <- str_extract(line, "(?<=Institution: ).+$") %>% str_trim()
headers <- NULL  # Reset headers for a new institution
} else if (!is.null(institution)) {
# Detect headers (columns like "Incidents Jan20 Feb20 ...")
if (is.null(headers) && str_detect(line, "\\s{2,}")) {
headers <- str_split(line, "\\s{2,}", simplify = TRUE) %>% as.character() %>% str_trim()
} else if (!is.null(headers)) {
# Align rows to headers
values <- str_split(line, "\\s{2,}", simplify = TRUE) %>% as.character() %>% str_trim()
if (length(values) == length(headers)) {
rows <- append(rows, list(c(Institution = institution, set_names(values, headers))))
}
}
}
}
# Step 4: Combine rows into a single tibble
if (length(rows) > 0) {
data <- bind_rows(rows)
}
head(data)
# Load libraries
library(pdftools)
library(tidyverse)
library(openxlsx)
# Step 1: Read the PDF file
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"  # Update with your exact file path
pdf_text <- pdf_text(file_path)  # Extract text from the PDF
# Step 2: Parse text into lines
lines <- unlist(str_split(pdf_text, "\\n"))  # Split text by lines
# Step 3: Initialize variables
data <- tibble()  # To store final output
institution <- NULL  # Placeholder for current institution
headers <- NULL  # Placeholder for detected headers
rows <- list()  # List to hold all row data
# Step 4: Process lines to extract institution and data
for (line in lines) {
# Detect institution and reset headers
if (str_detect(line, "COMPSTAT Incident Report - Institution:")) {
institution <- str_extract(line, "(?<=Institution: ).+$") %>% str_trim()
headers <- NULL  # Reset headers for a new institution
cat("Institution detected:", institution, "\n")  # Debugging: Check detected institution
} else if (!is.null(institution)) {
# Detect headers (columns like "Incidents Jan20 Feb20 ...")
if (is.null(headers) && str_detect(line, "\\s{2,}")) {
headers <- str_split(line, "\\s{2,}", simplify = TRUE) %>% as.character() %>% str_trim()
cat("Headers detected:", headers, "\n")  # Debugging: Check detected headers
} else if (!is.null(headers)) {
# Align rows to headers
values <- str_split(line, "\\s{2,}", simplify = TRUE) %>% as.character() %>% str_trim()
if (length(values) == length(headers)) {
# Append to rows only when lengths match
rows <- append(rows, list(c(Institution = institution, set_names(values, headers))))
} else {
# Log mismatched rows for manual review
cat("Mismatched row:", line, "\n")
}
}
}
}
# Step 5: Combine rows into a tibble
if (length(rows) > 0) {
# Ensure each element in rows is converted to a tibble
data <- bind_rows(lapply(rows, as_tibble))
}
# Step 6: Verify and save results
cat("Data extraction completed. Number of rows:", nrow(data), "\n")
head(data)
# Load libraries
library(pdftools)
library(tidyverse)
library(openxlsx)
# Load libraries
library(pdftools)
library(tidyverse)
library(openxlsx)
# Step 1: Read the PDF file
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"  # Update with your exact file path
pdf_text <- pdf_text(file_path)  # Extract text from the PDF
# Step 2: Parse text into lines
lines <- unlist(str_split(pdf_text, "\\n"))  # Split text by lines
# Step 3: Initialize variables
data <- tibble()  # To store final data
institution <- NULL  # Placeholder for the current institution
# Step 4: Process each line
for (line in lines) {
# Detect institution header
if (str_detect(line, "COMPSTAT Incident Report - Institution:")) {
institution <- str_extract(line, "(?<=Institution: ).+$") %>% str_trim()
} else if (!is.null(institution) && str_trim(line) != "") {
# Split the line by large spaces into individual values
values <- str_split(line, "\\s{2,}", simplify = FALSE)[[1]] %>% str_trim()
# Append each value as its own row
for (value in values) {
data <- bind_rows(data, tibble(Institution = institution, Value = value))
}
}
}
# Step 5: Verify and save results
cat("Data extraction completed. Number of rows:", nrow(data), "\n")
head(data)
# Load libraries
library(pdftools)
library(tidyverse)
library(openxlsx)
# Step 1: Read the PDF file
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"  # Update with your exact file path
pdf_text <- pdf_text(file_path)  # Extract text from the PDF
# Step 2: Parse text into lines
lines <- unlist(str_split(pdf_text, "\\n"))  # Split text by lines
# Step 3: Initialize variables
data <- tibble()  # To store final data
institution <- NULL  # Placeholder for the current institution
headers <- NULL  # Placeholder for detected headers
# Step 4: Process each line
for (line in lines) {
# Detect institution
if (str_detect(line, "COMPSTAT Incident Report - Institution:")) {
institution <- str_extract(line, "(?<=Institution: ).+$") %>% str_trim()
cat("Institution detected:", institution, "\n")  # Debugging
} else if (!is.null(institution)) {
# Detect headers (e.g., "Incidents Jan20 Feb20 ...")
if (is.null(headers) && str_detect(line, "\\bIncidents\\b")) {
headers <- str_split(line, "\\s+", simplify = TRUE) %>% as.character()
cat("Headers detected:", headers, "\n")  # Debugging
} else if (!is.null(headers)) {
# Parse data rows under headers
values <- str_split(line, "\\s+", simplify = TRUE) %>% as.character()
if (length(values) == length(headers)) {
# Align values with headers and append to data
data <- bind_rows(data, as_tibble(c(Institution = institution, set_names(values, headers))))
} else {
# Log mismatched rows
cat("Skipping mismatched row:", line, "\n")
}
}
}
}
# Step 5: Save the processed data to Excel
output_folder <- "D:/CDCR Data/Compstat Overall/2020/"
write.xlsx(data, file = paste0(output_folder, "COMPSTAT_2020_Data_Parsed.xlsx"), row.names = FALSE)
head(data)
# Load libraries
library(pdftools)
library(tidyverse)
library(openxlsx)
# Step 1: Read the PDF file
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"  # Update with your exact file path
pdf_text <- pdf_text(file_path)  # Extract text from the PDF
# Step 2: Parse text into lines
lines <- unlist(str_split(pdf_text, "\\n"))  # Split text by lines
# Step 3: Initialize variables
data <- list()  # To store rows of data
institution <- NULL  # Current institution
headers <- NULL  # Placeholder for detected headers
# Step 4: Process each line to detect headers and rows
for (line in lines) {
# Detect institution
if (str_detect(line, "COMPSTAT Incident Report - Institution:")) {
institution <- str_extract(line, "(?<=Institution: ).+$") %>% str_trim()
cat("Institution detected:", institution, "\n")  # Debugging
} else if (!is.null(institution)) {
# Detect headers (columns like "Incidents Jan20 Feb20 ...")
if (is.null(headers) && str_detect(line, "\\bIncidents\\b")) {
headers <- c("Institution", str_split(line, "\\s+", simplify = TRUE) %>% as.character())
cat("Headers detected:", headers, "\n")  # Debugging
} else if (!is.null(headers)) {
# Parse data rows under headers
values <- str_split(line, "\\s+", simplify = TRUE) %>% as.character()
if (length(values) + 1 == length(headers)) {  # Ensure alignment with headers (+1 for Institution column)
# Combine institution and row values
row <- c(Institution = institution, values)
data <- append(data, list(row))
} else {
cat("Skipping mismatched row:", line, "\n")  # Debugging
}
}
}
}
# Step 5: Combine all rows into a single tibble
if (length(data) > 0) {
data_tibble <- bind_rows(lapply(data, function(x) as_tibble(x)))
} else {
data_tibble <- tibble()
}
View(data_tibble)
head(data_tibble)
# Load libraries
library(pdftools)
library(tidyverse)
library(openxlsx)
# Step 1: Read the PDF file
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"  # Update with your exact file path
pdf_text <- pdf_text(file_path)  # Extract text from the PDF
# Step 2: Parse text into lines
lines <- unlist(str_split(pdf_text, "\\n"))  # Split text by lines
# Step 3: Initialize variables
data <- list()  # To store parsed rows
institution <- NULL  # Current institution
headers <- NULL  # Placeholder for headers
# Step 4: Process each line
for (line in lines) {
# Detect institution
if (str_detect(line, "COMPSTAT Incident Report - Institution:")) {
institution <- str_extract(line, "(?<=Institution: ).+$") %>% str_trim()
cat("Institution detected:", institution, "\n")  # Debugging
} else if (!is.null(institution)) {
# Detect headers
if (is.null(headers) && str_detect(line, "\\bIncidents\\b")) {
headers <- c("Institution", str_split(line, "\\s+", simplify = TRUE) %>% as.character())
cat("Headers detected:", headers, "\n")  # Debugging
} else if (!is.null(headers)) {
# Parse rows beneath headers
values <- str_split(line, "\\s+", simplify = TRUE) %>% as.character()
if (length(values) + 1 == length(headers)) {  # Match number of values to headers (+1 for Institution column)
row <- set_names(c(institution, values), headers)  # Align values to headers
data <- append(data, list(row))  # Append properly structured row
} else {
cat("Skipping mismatched row:", line, "\n")  # Debugging
}
}
}
}
# Step 5: Combine rows into a tibble
if (length(data) > 0) {
data_tibble <- bind_rows(data)  # Combine all rows into a tibble
} else {
data_tibble <- tibble()  # Return empty tibble if no rows processed
}
head(data_tibble)
head(data)
# Install required libraries (if not already installed)
if (!requireNamespace("pdftools", quietly = TRUE)) install.packages("pdftools")
if (!requireNamespace("tidyverse", quietly = TRUE)) install.packages("tidyverse")
if (!requireNamespace("openxlsx", quietly = TRUE)) install.packages("openxlsx")
# Load libraries
library(pdftools)
library(tidyverse)
library(openxlsx)
# Step 1: Read the PDF file
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"  # Update with your exact file path
pdf_text <- pdf_text(file_path)  # Extract text from the PDF
# Step 2: Parse text into lines
lines <- unlist(str_split(pdf_text, "\\n"))  # Split text by lines
# Step 3: Initialize variables
data <- list()  # To store parsed rows as named vectors or tibbles
institution <- NULL  # Current institution
headers <- NULL  # Placeholder for headers
# Step 4: Process each line
for (line in lines) {
# Detect institution
if (str_detect(line, "COMPSTAT Incident Report - Institution:")) {
institution <- str_extract(line, "(?<=Institution: ).+$") %>% str_trim()
cat("Institution detected:", institution, "\n")  # Debugging
} else if (!is.null(institution)) {
# Detect headers
if (is.null(headers) && str_detect(line, "\\bIncidents\\b")) {
headers <- c("Institution", str_split(line, "\\s+", simplify = TRUE) %>% as.character())
cat("Headers detected:", headers, "\n")  # Debugging
} else if (!is.null(headers)) {
# Parse rows beneath headers
values <- str_split(line, "\\s+", simplify = TRUE) %>% as.character()
if (length(values) + 1 == length(headers)) {  # Match number of values to headers (+1 for Institution column)
row <- set_names(c(institution, values), headers)  # Align values to headers
data <- append(data, list(as_tibble(row)))  # Convert row to tibble and append
} else {
cat("Skipping mismatched row:", line, "\n")  # Debugging
}
}
}
}
# Step 5: Combine rows into a tibble
if (length(data) > 0) {
data_tibble <- bind_rows(data)  # Combine all tibbles into one
} else {
data_tibble <- tibble()  # Return empty tibble if no rows processed
}
# Step 6: Save to Excel
output_folder <- "D:/CDCR Data/Compstat Overall/2020/"
write.xlsx(data_tibble, file = paste0(output_folder, "COMPSTAT_2020_Aligned.xlsx"), row.names = FALSE)
# Load libraries
library(pdftools)
library(tidyverse)
library(openxlsx)
# Step 1: Read the PDF file
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"  # Update with your exact file path
pdf_text <- pdf_text(file_path)  # Extract text from the PDF
# Step 2: Parse text into lines
lines <- unlist(str_split(pdf_text, "\\n"))  # Split text by lines
# Step 3: Initialize variables
data <- list()  # To store parsed rows as named vectors or tibbles
institution <- NULL  # Current institution
headers <- NULL  # Placeholder for headers
# Step 4: Process each line
for (line in lines) {
# Detect institution
if (str_detect(line, "COMPSTAT Incident Report - Institution:")) {
institution <- str_extract(line, "(?<=Institution: ).+$") %>% str_trim()
cat("Institution detected:", institution, "\n")  # Debugging
} else if (!is.null(institution)) {
# Detect headers
if (is.null(headers) && str_detect(line, "\\bIncidents\\b")) {
headers <- c("Institution", str_split(line, "\\s+", simplify = TRUE) %>% as.character())
cat("Headers detected:", headers, "\n")  # Debugging
} else if (!is.null(headers)) {
# Parse rows beneath headers
values <- str_split(line, "\\s+", simplify = TRUE) %>% as.character()
if (length(values) + 1 == length(headers)) {  # Match number of values to headers (+1 for Institution column)
row <- set_names(c(institution, values), headers)  # Align values to headers
data <- append(data, list(as_tibble(row)))  # Convert row to tibble and append
} else {
cat("Skipping mismatched row:", line, "\n")  # Debugging
}
}
}
}
# Step 5: Combine rows into a tibble with proper alignment
if (length(data) > 0) {
# Ensure each row aligns with headers and is converted to a tibble
aligned_rows <- lapply(data, function(x) {
# Ensure 'x' has the same length as headers
if (length(x) == length(headers)) {
set_names(x, headers)  # Align data to column names
} else {
NULL  # Skip rows that don't match headers
}
})
# Filter out NULL values and bind into a single tibble
data_tibble <- bind_rows(aligned_rows)
} else {
data_tibble <- tibble()  # Return empty tibble if no rows processed
}
head(data_tibble)
# Load necessary libraries
library(tidyverse)
# Step 1: Read the data (replace file_path with your actual file)
file_path <- "your_file_path.txt"
raw_lines <- read_lines(file_path)
# Step 1: Read the data (replace file_path with your actual file)
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"
raw_lines <- read_lines(file_path)
problems()
# Debugging Step 1: Print the first few lines to check the raw data structure
print("Raw lines of data:")
print(head(raw_lines))
# Step 2: Split the data into rows based on blank lines
data <- split(raw_lines, cumsum(raw_lines == ""))  # Group by blank-line delimiters
data <- lapply(data, function(x) x[x != ""])      # Remove blank entries from each group
# Debugging Step 2: Print how many rows have been captured
print(paste("Number of rows captured:", length(data)))
# Step 3: Create column headers (adjust as needed for your dataset)
headers <- c("Institution", "Incidents", "Jan20", "Feb20", "Mar20", "Apr20",
"May20", "Jun20", "Jul20", "Aug20", "Sep20", "Oct20", "Nov20", "Dec20", "Jan21")
# Debugging Step 3: Print the headers for review
print("Headers:")
print(headers)
# Step 4: Process each row
data <- lapply(data, function(row) {
# Attempt to split and clean each row into values
values <- unlist(str_split(row, "\\s+"))
# Debugging Step 4: Print each row's values for inspection
print("Row values:")
print(values)
# Return cleaned rows
values
})
# Load necessary libraries
library(tidyverse)
# Step 1: Read the data (replace file_path with your actual file)
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"
raw_lines <- read_lines(file_path)
problems(dat)
problems()
# Step 1: Read the data (replace file_path with your actual file)
file_path <- "D:/CDCR Data/Compstat Overall/2020/2020.pdf"
raw_lines <- read_lines(file_path)
# Debugging Step 1: Print the first few lines to check the raw data structure
print("Raw lines of data:")
print(head(raw_lines))
# Step 2: Split the data into rows based on blank lines
data <- split(raw_lines, cumsum(raw_lines == ""))  # Group by blank-line delimiters
data <- lapply(data, function(x) x[x != ""])      # Remove blank entries from each group
# Debugging Step 2: Print how many rows have been captured
print(paste("Number of rows captured:", length(data)))
# Step 3: Create column headers (adjust as needed for your dataset)
headers <- c("Institution", "Incidents", "Jan20", "Feb20", "Mar20", "Apr20",
"May20", "Jun20", "Jul20", "Aug20", "Sep20", "Oct20", "Nov20", "Dec20", "Jan21")
# Debugging Step 3: Print the headers for review
print("Headers:")
print(headers)
# Step 4: Process each row
data <- lapply(data, function(row) {
# Attempt to split and clean each row into values
values <- unlist(str_split(row, "\\s+"))
# Debugging Step 4: Print each row's values for inspection
print("Row values:")
print(values)
# Return cleaned rows
values
})
# Step 5: Combine rows into a tibble with debugging
if (length(data) > 0) {
# Debugging Step 5a: Check the lengths of rows compared to headers
row_lengths <- lapply(data, function(x) length(x))  # Get lengths of all rows
expected_length <- length(headers)  # Number of expected columns
print("Row lengths:")
print(row_lengths)  # Print lengths for review
print(paste("Expected length:", expected_length))
# Debugging Step 5b: Print mismatched rows
aligned_rows <- lapply(data, function(x) {
if (length(x) == expected_length) {
set_names(x, headers)  # Properly align the row with column names
} else {
print(paste("Skipping mismatched row:", toString(x)))  # Print problematic row
NULL  # Skip rows with mismatched lengths
}
})
# Step 6: Combine rows into a tibble
# Filter out NULL values (mismatched rows) and bind the valid ones
aligned_rows <- Filter(Negate(is.null), aligned_rows)  # Remove NULL rows
data_tibble <- bind_rows(aligned_rows)
} else {
data_tibble <- tibble()  # Return an empty tibble if no rows processed
}
# Step 7: Inspect the output
print("Final tibble:")
print(head(data_tibble))
library(tibble)
library(dplyr)
# Load necessary libraries
library(tidyverse)
# Step 1: Read the data (replace file_path with your actual file)
file_path <- "your_file_path.txt"
# Load necessary libraries
library(tidyverse)
# Step 1: Read the data (replace file_path with your actual file)
file_path <-  "D:/CDCR Data/Compstat Overall/2020/2020.pdf"
raw_lines <- read_lines(file_path)
# Debugging Step 1: Print the first few lines to check the raw data structure
print("Raw lines of data:")
print(head(raw_lines))
# Step 2: Split the data into rows based on blank lines
data <- split(raw_lines, cumsum(raw_lines == ""))  # Group by blank-line delimiters
data <- lapply(data, function(x) x[x != ""])      # Remove blank entries from each group
# Debugging Step 2: Print how many rows have been captured
print(paste("Number of rows captured:", length(data)))
# Step 3: Create column headers (adjust as needed for your dataset)
headers <- c("Institution", "Incidents", "Jan20", "Feb20", "Mar20", "Apr20",
"May20", "Jun20", "Jul20", "Aug20", "Sep20", "Oct20", "Nov20", "Dec20", "Jan21")
# Debugging Step 3: Print the headers for review
print("Headers:")
print(headers)
# Step 4: Process each row
data <- lapply(data, function(row) {
# Attempt to split and clean each row into values
values <- unlist(str_split(row, "\\s+"))
# Debugging Step 4: Print each row's values for inspection
print("Row values:")
print(values)
# Return cleaned rows
values
})
library(tibble)
library(dplyr)
# Step 5: Combine rows into a tibble with debugging and handling mismatched rows
if (length(data) > 0) {
# Debugging Step 5a: Check the lengths of rows compared to headers
row_lengths <- lapply(data, function(x) length(x))  # Get lengths of all rows
expected_length <- length(headers)  # Number of expected columns
print("Row lengths:")
print(row_lengths)  # Print lengths for review
print(paste("Expected length:", expected_length))
# Process rows: Normalize lengths of all rows
aligned_rows <- lapply(data, function(row) {
if (length(row) < expected_length) {
# Pad short rows with NAs
padded_row <- c(row, rep(NA, expected_length - length(row)))
print(paste("Padded row:", toString(padded_row)))  # Debugging: Check padded row
set_names(padded_row, headers)
} else if (length(row) > expected_length) {
# Truncate long rows
truncated_row <- row[1:expected_length]
print(paste("Truncated row:", toString(truncated_row)))  # Debugging: Check truncated row
set_names(truncated_row, headers)
} else {
# Properly aligned rows
set_names(row, headers)
}
})
# Debugging Step 5b: Filter out completely invalid rows (length 0)
aligned_rows <- Filter(function(x) !all(is.na(x)), aligned_rows)
# Step 6: Combine rows into a tibble
data_tibble <- bind_rows(aligned_rows)
} else {
# If no rows processed, return an empty tibble
data_tibble <- tibble()
}
# Final Debugging: Check the resulting tibble
print("Final tibble:")
print(head(data_tibble))
